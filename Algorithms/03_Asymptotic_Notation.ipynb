{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual running time of a program depends on the underlying hardware, the interpreter/compiler, etc. It would be helpful to think about algorithmic efficiency more abstractly. We can use two key factors:\n",
    "\n",
    "1. The size of the input\n",
    "2. Rate of growth with respect to input size\n",
    "\n",
    "For (2), we need to simplify things a bit to focus on only the most influential parts of the algorithm. If we were to express running time as the polynomial $T(n) = 6n^2 + 100n + 300$, where $n$ is the size of the input, the first term has the highest order, and so increases most quickly with respect to $n$. Compared to the rate of increase of $6n^2$, $100n$ is relatively inconsequential. We could simplify that polynomial to say that the running time of that algorithm grows at a rate on the order of $n^2$, dropping the coefficient and the lower-order terms.\n",
    "\n",
    "When we drop the constant, the coefficients, and the lower-order terms, we are using **asymptotic notation**, expressed in three ways: big-&Theta;, big-O, and big-&Omega;.\n",
    "\n",
    "# Big-&Theta; Notation\n",
    "\n",
    "## Example: Linear search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "var doLinearSearch = function(array, targetValue) {\n",
    "    for (var guess = 0; guess < array.length; guess++) {\n",
    "        if (array[guess] === targetValue) {\n",
    "            return guess;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return -1\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of times that the for-loop will run is $n$, which is `array.length`. This is the worst-case scenario, and occurs when `targetValue` is not in `array`.\n",
    "\n",
    "Each time the loop iterates, it does three things:\n",
    "\n",
    "1. Compares `guess` with `array.length`\n",
    "2. Compares `array[guess]` with `targetValue`\n",
    "3.    \n",
    "    1. Returns `guess` or\n",
    "    2. Increments `guess`\n",
    "    \n",
    "We can reasonably argue that each step takes a constant amount of time each time it runs. If the loop runs all $n$ times, then the time for all $n$ iterations is $c_1n$, where $c_1$ is the sum of the running times of the computations in one loop iteration. It doesn't really matter what $c_1$ is, and we can't really tell what it is without detailed measurement anyway. \n",
    "\n",
    "There's also some amount of overhead, such as initializing the looping variable `guess` and possibly returning `-1` at the end. We could denote this overhead as $c_2$, and express the total worst-case run time for this linear search as $c_1n + c_2$.\n",
    "\n",
    "The constant factor $c_1$ and the lower-order term $c_2$ don't really tell us much about the rate of increase in the running time, so we can drop those terms. The significant part is that the worst-case running time of linear search grows linearly with the array size. We can denote this as $\\Theta(n)$.\n",
    "\n",
    "When we say that the running time is $\\Theta(n)$, we're saying that after some minimum threshold value of $n$, the running time is at least $k_1n$ and at most $k_2n$ for some constants $k_1$ and $k_2$. That is, the running time is bounded by constant multiples of $n$.\n",
    "\n",
    "We can express other growth factors in big-&Theta; notation, not just $n$. It could be some function  of $n$, such as $n^2$, $\\log n$, or any arbitrary $f(n)$. No matter what function we use, big-&Theta; notation means that after some threshold value, the running time will be bounded by $k_1f(n)$ and $k_2f(n)$.\n",
    "\n",
    "## Functions in Big-&Theta; Notation\n",
    "\n",
    "If an algorithm takes constant time, regardless of its input size, we say that the run time is $\\Theta(n^0)$ or $\\Theta(1)$. This is because the highest order term in the polynomial of the run time is 0, that is $T(n) = c_1$. Dropping all of the lower order terms and the coefficients leaves us with $n^0 = 1$.\n",
    "\n",
    "If an algorithm took $\\Theta(\\log_{10} n)$ time, we could also correctly say that it took $\\Theta(\\log_2 n)$ time. That's because the base of a logarithm is a constant, and so effectively drops out. Because of this property, we can say that the worst-case running time of a binary search is $\\Theta(\\log_a n)$ for any positive constant $a$. We know that the worst case number of guesses is $\\log_2 n + 1$, with other constant-time factors such as setup and returning. Those constants drop out, and the base is arbitrary. We often refer to $\\Theta(\\log_2 n)$ because base-2 logarithms and powers of 2 are natural in computer science.\n",
    "\n",
    "We can order different running times based on big-&Theta; notation. If $a$ and $b$ are constants and $a < b$, we know that $\\Theta(n^a)$ grows more slowly than $\\Theta(n^b)$. Similarly, logarithms grow more slowly than polynomials. $\\Theta(\\log_2 n)$ grows more slowly than $\\Theta(n^a)$ for any positive constant $a$. But $\\Theta(\\log_2 n)$ grows faster than $\\Theta(1)$.\n",
    "\n",
    "Ordered from slowest to fastest growing, some of the most common functions of growth we will see are:\n",
    "\n",
    "1. $\\Theta(1)$\n",
    "2. $\\Theta(\\log_2 n)$\n",
    "3. $\\Theta(n)$\n",
    "4. $\\Theta(n\\log_2 n)$\n",
    "5. $\\Theta(n^2)$\n",
    "6. $\\Theta(n^2\\log_2 n)$\n",
    "7. $\\Theta(n^3)$\n",
    "8. $\\Theta(2^n)$\n",
    "9. $\\Theta(n!)$\n",
    "\n",
    "# Big-O Notation\n",
    "\n",
    "We use big-&Theta; notation to bound asymptotic growth from both above and below. Big-O notation allows us to bound running time only from above -- the worst-case scenario.\n",
    "\n",
    "The worst-case running time of binary search is $\\Theta(\\log_2 n)$, but it is incorrect to say that binary search runs in $\\Theta(\\log_2 n)$ time in _all_ cases. If the target value is found on the first guess, then it runs in $\\Theta(1)$ time. It never gets worse than $\\Theta(\\log_2 n)$, but it can be better.\n",
    "\n",
    "Big-O notation lets us describe the worst-case scenario, knowing that it is almost certainly not the same as the best case and possibly not even the average case. If a running time is $O(f(n))$, then for large enough $n$, the running time is at most $kf(n)$ for some constant $k$. Big-O notation provides an **asymptotic upper bound** for the rate of growth of an algorithm. If we say that a running time is $\\Theta(f(n))$ for some situation, then it is also $O(f(n))$. The worst case run time of binary search is $\\Theta(\\log_2 n)$, and also $O(\\log_2 n)$. _But the converse is not necessarily true._ Binary search always runs in $O(\\log_2 n)$ time, but it does _not_ always run in $\\Theta(\\log_2 n)$ time. Big-O notation provides only an upper bound, but that may not always be precise. It is valid to say that binary search runs in $O(n)$ time, because it will never grow faster than a constant times $n$. But that's not terribly informative, since we know that a tighter upper bound is $O(\\log_2 n)$.\n",
    "\n",
    "# Big &Omega; Notation\n",
    "\n",
    "We use big-&Omega; notation to say that an algorithm takes _at least_ a certain amount of time, but providing no information about the upper bound. If a running time is $\\Omega(f(n))$, then for large enough $n$, the running time is at least $kf(n)$ for some constant $k$.\n",
    "\n",
    "Big-&Omega; notation provides asymptotic lower bounds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "10.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
